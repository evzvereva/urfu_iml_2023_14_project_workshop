# Серверная часть проекта "Умный городской гид по Екатеринбургу"

Предоставлят API к большим языковым моделям (LLM) при помощи YandexGPT и Ollama. Является частью проекта "Умный городской гид по Екатеринбургу" и создан в рамках Проектной практики Подгруппы 14.

Сайт проекта доступен по [ссылке](https://urfu-iml-2023-14-project-workshop.streamlit.app/).

[YandexGPT](https://cloud.yandex.ru/services/yandexgpt) - находящаяся на этапе тестирования нейросеть семейства GPT от компании «Яндекс», которая генерирует тексты на основе данных из интернета.

[Ollama](https://ollama.ai/) - библиотека с открытым исходным кодом, которая позволяет запускать различные модели LLM на своих серверах. В данном проекте выбрана модель [Starling-7B](https://starling.cs.berkeley.edu/). Модель использует возможности нового набора ранжирующих данных с маркировкой GPT-4, Nectar, а также нового конвейера обучения вознаграждениям и настройки политики. Starling-7B набирает 8,09 балла в MT Bench с GPT-4 в качестве судьи, превосходя все существующие на сегодняшний день модели в MT-Bench, за исключением GPT-4 и GPT-4 Turbo от OpenAI.

## Конечные точки API

- Корневая точка
- Документация (Swagger)
- Чат (формирование ответа пользователю, выполнение сервисных операций)

## Корневая точка

```shell
GET /
```

Возвращает пустую строку. Требуется для проверки статуса сервера.

## Документация (Swagger)

```shell
GET /docs
```

## Чат (формирование ответа пользователю)

```shell
POST /chat
```

Формируется ответ пользователю с использованием больших языковых моделей YandexGPT и Ollama. Имеется возможность передачи предыдущих сообщений для учета контекста общения.

Если не заданы параметры испольования модели в запросе, то в первую очередь для формирования ответа используется API сервиса YandexGPT. В качестве резервной используется API библиотеки Ollama. Если при обращении к YandexGPT возникает ошибка (сейчас сервис работает в режиме Preview и иногда бывает недоступен, также в сервисе установлено ограничение на количество запросов в час), то задействуется сервер Ollama, который развернут в отдельном контейнере Docker.

### Параметры

- `api_key`: (обязатеное поле) токен для авторизации
- `prompt`: (обязатеное поле) текст запроса пользователя
- `history`: сообщения чата в виде массива, используется для передачи истории общения, каждый элемент массива состоит из двух полей:
    * `role` - роль, возможные значения: `system`, `user`, `assistant`
    * `content` - текст сообщения, запрос для роли `user`, ответ для роли `assistant` и системный запрос для роли `system`
- `options`: параметры модели, состоит из полей:
    * `model` - имя модели; если не заполнено, то используется YandexGPT
    * `embeddings` - параметры использования embeddings, возможные значения: `prompt`, `search_docs`, `create_vs`

В параметре `options` можно явно задать использование сервера Ollama, для этого в поле `model` должно быть передано значение `ollama`.
Если в `options` передано значение `ollama`, то анализируется параметр `embeddings`.
При передаче в `embeddings` значения `create_vs` запускается создание или обновление, если оно было создано ранее, вектроного хранилища. В качестве векторной СУБД исользуется ChromaDB. Для заполнения векторного хранилища анализируются все текстовые файлы в каталоге `text`.
При передаче в `embeddings` значения `search_docs` выполняется поиск документов в векторном хранилище по фразе из поля `prompt`.
При передаче в `embeddings` значения `prompt` формируется ответ пользователю на основе контекста содержащегося в документах, которые соответствуют фразе из поля `prompt`.

### Примеры

#### Запрос

Отправка простого запроса пользователя.

```json
{
  "api_key": "API_KEY",
  "prompt": "Расскажи про город",
  "history": []
}
```

#### Ответ

```json
{
  "answer": "Екатеринбург — административный центр Свердловской области России. Город расположен на восточном склоне Среднего Урала, на реке Исеть (притоке реки Тобол). Екатеринбург — один из крупнейших городов России, пятый по численности населения (1 419 972 человек на 1 января 2023 года) в её границах.\n\nЕкатеринбург — важный промышленный, культурный и научный центр Урала. В городе расположены более 700 предприятий чёрной и цветной металлургии, машиностроения, химической, лёгкой и пищевой промышленности. Екатеринбург является важным транспортным узлом: через город проходят железнодорожные и автомобильные магистрали, а также река Исеть, по которой осуществляется водное сообщение.\n\nЕкатеринбург — один из образовательных и научных центров России. В городе расположены несколько десятков высших учебных заведений, в том числе Уральская государственная горно-геологическая академия, Уральский государственный университет имени первого Президента России Б. Н. Ельцина, Уральский федеральный университет имени первого Президента России Б. Н. Ельцина и другие.\n\nЕкатеринбург — крупный культурный центр Урала. В городе работают несколько театров, филармония, музеи изобразительных искусств, литературный музей Ф. М. Решетникова, музей истории и археологии Урала и другие.\n\nЕкатеринбург — один из самых популярных городов-туристов в России. В городе много памятников архитектуры и истории, а также природных достопримечательностей, таких как озеро Шарташ и парк «Зеленая Роща».\n\nЕкатеринбург — город с богатой историей, восходящей к началу XVIII века. В 1723 году на берегу реки Исети был основан Екатеринбургский завод — один из старейших металлургических заводов России. В 1878 году Екатеринбург получил статус города.\n\nЕкатеринбург является членом Евразийского экономического союза и Уральского федерального округа."
}
```

#### Запрос

Создание или обновление вектроного хранилища.

```json
{
  "api_key": "API_KEY",
  "prompt": "",
  "history": [],
  "options": {
    "model": "ollama",
    "embeddings": "create_vs"
  }
}
```

#### Ответ

```json
{
  "answer": "created"
}
```


## Настройка

### Файл настроек

Для работы серверной части проекта необходим файл с настройками `settings.yaml`. Структура файла:

```yaml
yandexGPT:
  url: https://llm.api.cloud.yandex.net/foundationModels/v1/completion
  api_key: 
  folder: 
  system_template: 
GPT4All:
  model_path: ./models
  model_name: starling-lm-7b-alpha.Q4_0.gguf
  device: gpu
  system_template: 
  promt_template: "GPT4 User: {0}<|end_of_turn|>GPT4 Assistant:"
ollama:
  url: http://ollama:11434/api/chat
  base_url: http://ollama:11434
  model: starling-lm
  system_template: 

```

В разделе `yandexGPT` указываются параметры доступа к сервису YandexGPT (`url`, `api_key`, `folder`) и системный запрос (`system_template`) для задания параметров ответов языковой модели. Системный запрос можно не указывать, тогда используется системный запрос по умолчанию, который задан в коде.

В разделе `GPT4All` указываются параметры работы библиотеки GPT4All:
  - `model_path` - путь к каталогу, где хранятся файлы моделей,
  - `model_name` - имя используемой модели,
  - `device` - используемое устройство, возможны 2 значения: `cpu` и `gpu`,
  - `system_template` - системный запрос,
  - `promt_template` - шаблон запросов, может меняться в зависимости от выбранной модели.

В разделе `ollama` указываются параметры сервера Ollama, предполагается, что он будет будет развернут в контейнере Docker:
  - `url` - путь к методу чата сервера Ollama,
  - `base_url` - базуовый путь сервера Ollama, используется для работы с embeddings,
  - `model` - имя используемой модели,
  - `system_template` - системный запрос.

### Авторизация, файл secrets.txt

Для авторизации в API необходимо передавать токен в поле `api_key`. При получени запроса вычисляется хэш-сумма токена и проверяется наличие такого значения в файле `secrets.txt`. Если значение содержится в этом файле, то выполняется дальнейшая обработка запроса. Если такого значения нет в этом файле, то генерируется ошибка с HTTP статусом 401 (UNAUTHORIZED).

## Установка

Для создания образа Docker в репозитории находится файл Dockerfile. Чтобы запустить создание образа только одного сервера API, необходимо в каталоге `backend_api` выполнить следующую команду:
```shell
docker build -t gpt-api .
```

Но предполагается, что для работы будет использоваться docker compose. Чтобы автоматизировать развертывание в репозиториё добавлен файл `docker-compose.yaml`. Для его использования нужно в каталоге `backend_api` выполнить команду:
```shell
docker compose up
```
В результате выполнения этой команды будет собран образ сервера API, потом будет загружен образ сервера Ollama с сайта https://hub.docker.com и далее будут запущены контейнеры Docker с двумя серверами.
После этого сервер API будет готов к работе.

Перед созданием образов и запуском контейнеров необходимо в каталог `backend_api` добавить файлы `settings.yaml` и `secrets.txt`.
